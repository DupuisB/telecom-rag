# Title: Ideas - Que disent les chartes éthiques sur l’IA des acteurs publics, privés et ONG ?

## [Accueil](https://www.telecom-paris.fr "https://www.telecom-paris.fr") > [fr](https://www.telecom-paris.fr/fr "fr") > [Ideas](https://www.telecom-paris.fr/fr/ideas "Ideas") > [Que disent les chartes éthiques sur l’IA ?](https://www.telecom-paris.fr/fr/ideas/intelligence-artificielle-chartes-ethiques)

[](https://www.telecom-paris.fr/fr/accueil)

# Télécom Paris Ideas  
Les chartes éthiques sur l’IA des acteurs publics, privés, ONG

## Que disent les chartes éthiques sur l’IA des acteurs publics, privés, ONG ?  
L’apport des sciences sociales sur les sujets d’explicabilité et de régulation
de l’IA  

[Tiphaine Viard](https://www.telecom-paris.fr/tiphaine-viard), maîtresse de
conférences Numérique, Organisation et Société à Télécom Paris, mars 2023.

Mots-clés : éthique, régulation

[](https://www.telecom-paris.fr/wp-content-EvDsK19/uploads/2023/01/tiphaine-
viard-page-perso.jpg)Tiphaine Viard, Maîtresse de conférences « Numérique,
organisation et société » à Telecom Paris, répond à nos questions autour de
son étude sur les chartes éthiques des acteurs publics, privés et ONG.

Le sujet de la régulation de l’IA est beaucoup abordé ces derniers temps :
l’IA Act européen devrait voir le jour cette année et, de ChatGPT à
l’utilisation des algorithmes pour sécuriser les prochains JO, les sujets
d’éthique émergent de plus en plus dans le débat. Tiphaine Viard, maîtresse de
conférences de l’équipe Numérique, organisation et société, a consacré une
étude aux chartes éthiques des acteurs publics et privés pour analyser leurs
visions de l’éthique de l’IA et leurs définitions de l’IA de confiance.

Que peuvent nous apporter les sciences sociales sur les sujets d’explicabilité
et de régulation de l’intelligence artificielle ?

Propos recueillis par Isabelle Mauriac

## Podcast

Retrouvez cette interview en format audio dans le cadre des [podcasts Télécom
Paris Ideas](https://podcast.ausha.co/telecom-paris-ideas) :

Podcast Michel Desnoues, Télécom Paris

## Entretien

— Tiphaine, vous venez de terminer un travail sur les chartes éthiques des
acteurs publics et privés pour analyser leurs visions de l’éthique de l’IA et
leurs définitions de l’IA de confiance. Pourquoi avoir choisi ce sujet ?

Oui effectivement, avec Mélanie Gornet qui est doctorante à Télécom Paris
aussi, nous avons mené un travail d’analyse des chartes éthiques. Il y a
énormément d’acteurs, qu’ils soient privés, institutionnels, académiques
aussi, ou même des collectifs de citoyens ou des ONG, qui se positionnent sur
le sujet de l’éthique de l’IA en publiant des chartes ou des manifestes. Et on
imagine en général, quand on parle de manifestes, des documents assez courts,
non contractuels et souvent un peu vagues. Nous avons voulu à la fois vérifier
si c’était le cas sur le thème de l’IA et étudier en profondeur leur
positionnement, quels mots sont utilisés, quels concepts sont mobilisés et ce
que cela dit, en creux, du champ de l’éthique de l’intelligence artificielle.
Nous avons étudié 75 documents mais nous en avons collecté plusieurs
centaines…

— Alors, pour que ce soit un peu plus concret, pouvez-vous nous décrire quels
sont ces acteurs que vous avez étudiés, publics et privés ?

De mémoire, on a entre 25-30 % d’acteurs privés, vraiment industriels, séparés
en deux : à la fois les industriels de l’intelligence artificielle type
Google, DeepMind, Microsoft, IBM…, et des industriels d’autres domaines qui
utilisent de l’IA (par exemple dans le domaine bancaire pour de la maintenance
prédictive) mais qui ont des cas d’applications plus précis. Du côté des
institutionnels, ce sont plutôt des documents soit au niveau national (par
exemple le Conseil d’État a publié un rapport sur l’IA de confiance pour la
fonction publique) ou alors au niveau supranational. Typiquement l’Union
européenne qui travaille en ce moment à la suite du RGPD sur un document
législatif qui encadre l’utilisation et les définitions de l’intelligence
artificielle, l’AI Act.

Enfin, ce qui caractérise un peu notre corpus, c’est qu’on a aussi des
documents d’ONG qui en général sont très au point sur les questions de droits
fondamentaux posés par l’utilisation des intelligences artificielles, et même
des collectifs de citoyens qui ne sont liés que par leur intérêt pour le sujet
et souhaitent pas exemple aller vers une IA décoloniale ou une IA
politiquement plus comme ci ou comme ça.

— Oui… donc on voit que les approches sont diverses mais on voit aussi que
tous ces acteurs se sont aujourd’hui dotés de chartes. On a vraiment
l’impression que les problématiques de régulation prennent de l’ampleur ces
derniers mois, même si l’essor de l’IA et donc l’intérêt pour les sujets
d’éthique ne date pas d’hier… Pour nous aider à mettre tout cela en
perspective, pouvez-vous nous rappeler les évolutions marquantes et
l’actualité de ce sujet ?

Effectivement, les méthodes d’intelligence artificielle ont suscité de plus en
plus d’attention ces dernières années, notamment avec des avancées
mathématiques et informatiques qu’on regroupe souvent sous le terme de deep
learning, qui englobe à la fois la puissance de calcul, la disponibilité des
données et les méthodes d’optimisation mathématique qui permettent de calculer
sur des gros volumes de données en un temps raisonnable. Sachant que les
premières avancées techniques ne sont pas si récentes en fait et datent plutôt
des années 1990(…) mais depuis 2010 et après il y a vraiment une explosion des
modèles, des applications et des usages autour de l’intelligence artificielle.

Et donc de ce fait, conjointement à cela s’est posée la question de comment
réguler cela, quelles limites on veut mettre, que désire-t-on en tant que
citoyen, en tant que législateur(…) veut-on considérer qu’il y a des usages
inacceptables, veut-on encadrer certains usages ou au contraire partir du
principe que la technique s’encadrera toute seule… Il y a tout un champ de
possibles, de façon un peu similaire à ce qui s’est passé pour les réseaux
sociaux, avec une innovation technologique qui change énormément d’usages…

L’Union européenne, dans la foulée du succès du Règlement Général pour la
Protection des Données, a décidé de proposer un texte considéré comme cadre
régulant l’intelligence artificielle au sens large, avec toutes les
difficultés que cela peut impliquer de définir ce qu’est et n’est pas
l’intelligence artificielle.  
Ce document est actuellement en discussion animée afin de définir les usages
acceptables, les usages à interdire, quel que soit le contexte, ceux que l’on
voudrait encadrer, etc.

Il y a beaucoup d’applications qui suscitent des interrogations en matière de
régulation. Celles dont on a beaucoup parlé autour de ChatGPT ou sur la
surveillance vidéo au moment des Jeux Olympiques, ainsi que de nombreux cas
d’utilisation autour des véhicules autonomes et de la robotique. C’est aussi
le cas de sujets qui ont l’air moins saillants mais aussi importants sur les
outils automatisés (comme trier des candidatures, trier des demandes
d’allocations ou de prêts, par exemple) qui au final ont recours aussi au sens
large à des IA et sont peut-être moins flamboyants ou moins apparents, mais
sont tout aussi importants et ont des conséquences sociales tout aussi
réelles.

En tant que chercheur et chercheuse, on a un peu une double position, à la
fois de contribuer en termes de modèles mais aussi d’agir plus concrètement
sur le terrain. Par exemple, les chercheurs en sciences sociales chercheront à
rendre plus transparents les processus sociaux qui traversent ces espaces-là
ou à étudier la façon dont ces processus sont ou non démocratiques. Et de
l’autre, ce sera plus un travail en lien avec des ministères pour orienter les
textes de loi ou pour donner un avis éclairé sur tel ou tel projet.

— Si on en revient maintenant à votre travail… qu’avez-vous appris de
l’analyse des chartes éthiques ?

Déjà nous avons confirmé un peu nos a priori sur le fait que ces documents-là
sont souvent plutôt courts et non contractuels… La plupart d’entre eux (80% de
mémoire) font moins de 2 pages, même s’il y a des exceptions (comme celui de
la chambre des lords de Royaume-Uni qui fait plusieurs centaines de pages).  
Ils sont effectivement en général très vagues et ils utilisent un peu tous les
mêmes vocabulaire. Il y a des grands thèmes qui reviennent, notamment l’équité
algorithmique (fairness), des questions sur la vie privée et le respect des
droits fondamentaux entre autres.

Nous nous sommes rendu compte que le même vocabulaire ne voulait pas forcément
dire exactement la même chose, notamment parce qu’il y a beaucoup de
circulation d’informations. Par exemple le terme d’explicabilité, c’est à dire
la capacité d’expliquer un algorithme, est un terme d’origine informatique
venant vraiment du monde des entreprises et des labos de recherche.

Ce terme a progressivement été repris institutionnellement par le secteur
public qui prend maintenant le devant de la scène. Ainsi aujourd’hui, quand on
parle d’explication d’un algorithme d’intelligence artificielle, on pense tout
de suite au terme d’explicabilité mais sans forcément les mêmes implications.
Un ou une juriste, ou bien un ou une députée n’a pas forcément les mêmes
attentes en pensant explicabilité qu’une chercheuse en informatique par
exemple, qui a une idée qui est très claire de ce que ça veut dire en termes
de méthode mathématique. Ce côté polysémique est très intéressant.

Au-delà, nous nous sommes aussi rendu compte qu’il y avait des partis pris
assez récurrents et des sujets quasi systématiquement oubliés et absents de
ces documents. Pour donner quelques exemples, il y a typiquement le thème du
travail qui est beaucoup abordé dans ces chartes, mais avec une approche de la
façon dont l’intelligence artificielle a le potentiel de changer le monde du
travail dans les pays plutôt européens ou plutôt « premier monde » et sur la
façon dont cela bouscule des choses existantes. Mais les questions sur les
dynamiques de division injuste du travail, déjà existantes, ne sont quasiment
jamais abordées, notamment le fait de sous-traiter dans des pays du sud
global, le fait de bénéficier de données qui ont été publiées par d’autres
personnes qui elles n’en bénéficient pas. Ces questions sont notamment
traitées par mon collègue Antonio Casilli. Ce sujet-là est un sujet tristement
absent…

Un autre absent est l’écologie et l’impact environnemental de l’intelligence
artificielle qui, quand il est abordé, l’est de manière encore plus
superficielle que le reste, car il y a encore assez peu d’éléments techniques
concrets à donner, bien qu’il y ait des recherches. Ce sujet est peu abordé,
ainsi que le contexte social et environnemental qui pourrait inciter à limiter
voire s’abstenir d’utiliser d’intelligence artificielle.

Enfin un dernier grand résultat est que tous ces thèmes sont très peu «
opérationnalisés ». On est plutôt face à une liste de desiderata : l’IA doit
être au bénéfice de la société… on sait pas trop ce que ça veut dire mais
c’est difficile d’être contre ça. L’IA doit être efficace, l’IA doit être
soutenable … mais sans vraiment de traduction concrète.

On ne dit jamais par exemple « une IA soutenable signifie réguler le bilan
carbone des serveurs : donc pas plus de tant de kilos de CO2 pour un modèle »
ou le mettre en regard du nombre de personnes à qui elle bénéficie. Ces
questions sont délicates, mais ne pas les trancher peut mener à des conflits
de valeurs, qui commencent tout juste à être étudiés. On veut plus de
transparence, c’est très bien, mais on veut aussi protéger la vie privée des
gens… et ces deux besoins là se contredisent un petit peu l’un l’autre.
Comment veut-on arbitrer par exemple ? Ce sont des questions très
intéressantes mais complexes.

— Alors, pour caricaturer on pourrait dire qu’il y a presque autant de
dimensions de l’IA que d’acteurs ou en tous cas qu’il y a des différences
notables de perception… Comment peut-on de ce fait se situer par rapport à ces
notions de transparence, d’équité ou d’explicabilité ? Avez vous, en tant que
chercheurs, votre propre définition de l’IA de confiance ?

Une des conclusions de notre travail a été de dire que c’est difficile d’avoir
une définition très claire de l’IA de confiance. Déjà parce que c’est
difficile d’avoir une définition très claire de ce que c’est l’intelligence
artificielle tout court donc en rajoutant des mots ça devient plus compliqué.

Mais en même temps, de ce fait, cela renforce l’importance d’avoir une
diversité d’acteurs, d’actrices et de personnes impliquées qui vont pouvoir en
parler, donner leur avis et pouvoir peser dans les débats. Bien sûr, des
chercheurs, des chercheuses des professionnels en entreprises et dans les
labos privés connaissent les enjeux de façon très pointue, mais c’est aussi le
cas des personnes dans les ONG qui connaissent beaucoup plus le rapport aux
droits fondamentaux. Idéalement, cela devrait être aussi le cas des personnes
qui contribuent à nourrir et à étiqueter ces bases de données, en général très
souvent exclues des bénéfices des systèmes d’intelligence artificielle
ensuite, des législateurs, des juristes, etc.

À partir de tous ces regards croisés et de tous les dissensus et des consensus
que cela implique et de toute la complexité que cela apporte, je pense qu’on
pourra petit à petit faire émerger quelque chose qui, au lieu de renforcer les
dynamiques de pouvoir existantes, sera vraiment en mesure de les rééquilibrer,
ce qui me paraît souhaitable.

[](https://www.telecom-paris.fr/fr/ideas/sommaire)

