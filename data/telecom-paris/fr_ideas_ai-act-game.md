# Title: L’AI Act Game, un jeu pour tout savoir du règlement européen sur l’IA en 30 minutes

## [Accueil](https://www.telecom-paris.fr "https://www.telecom-paris.fr") > [fr](https://www.telecom-paris.fr/fr "fr") > [Ideas](https://www.telecom-paris.fr/fr/ideas "Ideas") > [L’AI Act Game, un jeu créé à Télécom Paris](https://www.telecom-paris.fr/fr/ideas/ai-act-game)

[](https://www.telecom-paris.fr/fr/accueil)

# Télécom Paris Ideas  
AI Act Game, un jeu créé à Télécom Paris

## L’AI Act Game, un jeu créé à Télécom Paris pour tout savoir du règlement
européen sur l’IA en 30 minutes  

[Thomas Le Goff](https://www.telecom-paris.fr/thomas-le-goff), maître de
conférences en droit et régulation du numérique à Télécom Paris, juin 2024.

Alors que l’[AI Act qui va réguler l’intelligence artificielle en
Europe](https://www.telecom-paris.fr/fr/ideas/regulation-intelligence-
artificielle) sera publié au Journal Officiel dans quelques semaines et
prévoit un déploiement en plusieurs étapes, le sujet devient très concret pour
les entreprises !

Pour les aider à s’acclimater au sujet et à mettre en place les mesures
adaptées, Thomas Le Goff, maître de conférences en droit et régulation du
numérique à Télécom Paris, a conçu un serious game, l’[AI Act
Game](https://www.canva.com/design/DAGF2FfogqE/QpjWW1ghhCr_GMZT_gLJ3A/view),
qui permet de rentrer dans les obligations du texte de façon ludique.

Thomas nous présente cet outil, et nous en profitons pour faire le point avec
lui sur les enjeux de l’entrée en vigueur de l’AI Act pour les entreprises.

Propos recueillis par Isabelle Mauriac

## AI Act Game : une approche ludique de l’AI Act

— Thomas, pouvez-vous nous résumer où en est l’entrée en vigueur de l’AI Act ?

Le texte est en cours d’adoption finale. Il a été adopté par les deux
institutions européennes, le Conseil de l’UE et le Parlement européen. On
attend sa publication officielle, ce qui lancera les délais pour l’entrée en
application de toutes ses dispositions.

L’AI Act entrera en vigueur 20 jours après sa publication au Journal Officiel,
donc vraisemblablement vers fin juillet 2024. Ensuite, on aura tout un
calendrier d’entrée en application des dispositions : la catégorie des
systèmes d’IA interdits le seront au bout de 6 mois, une deuxième échéance à 1
an verra la régulation des systèmes d’IA à usage général, puis ce seront les
systèmes d’IA à haut risque et les obligations de transparence au bout de deux
ans…. Toutes ces catégories, et les obligations en découlant, sont adressées
dans le jeu.

— Quel est l’objectif de l’AI Act Game que vous présentez cette semaine ?

[](https://www.canva.com/design/DAGF2FfogqE/QpjWW1ghhCr_GMZT_gLJ3A/view)On
peut dire que c’est un serious game, autrement dit un support pédagogique
ludique. Nous l’avions pensé initialement pour nos élèves. Mais on se rend
compte qu’il y a besoin de formation sur ce texte auprès de différents
publics, que ce soient les ingénieurs, les juristes, les équipes conformités,
les avocats… L’idée est de donner une première approche du texte avec ses
principales dispositions, qui soit accessible à tous, sans prérequis en termes
de bagage technique ou juridique, et de permettre une acclimatation assez
rapide à ce texte (il fait maintenant plus de quatre cents pages !), qui a
notamment été étoffé dans ses dernières versions par des mesures ciblant les
IA génératives (absentes de la première proposition de 2021). Le jeu permet de
s’y acclimater en 30 minutes.

— Vous me disiez avoir travaillé précédemment avec des directions juridiques
sur différents textes, comme le RGPD, avec des supports pédagogiques pour
former les équipes juridiques ou informatiques. Comme pour le RGPD, l’idée est
que les entreprises puissent s’en saisir et le déployer après dans leur
organisation ?

Oui, ce serious game pourra servir de support d’information, de
sensibilisation et même de formation. L’AI Act est un texte qui va avoir une
importance similaire à celle du RGPD en matière d’intelligence artificielle et
nous allons rentrer dans une période de mise en conformité de deux ans. Les
entreprises vont devoir s’en saisir dans un délai assez restreint. De plus,
six mois après l’entrée en vigueur du texte, les fournisseurs et utilisateurs
de systèmes d’IA devront avoir pris des mesures pour assurer « un niveau
suffisant de connaissances en matière d’IA à leur personnel et aux autres
personnes chargées du fonctionnement et de l’utilisation des systèmes d’IA »
(article 4 de l’AI Act). Les entreprises n’ont donc pas le choix que de
s’acclimater rapidement !

Pour en revenir au RGPD, il y a de nombreuses similitudes avec l’AI Act :  
D’abord, comme pour les « traitements de données à caractère personnel », la
première mission des entreprises va être de faire une cartographie de tous les
systèmes d’IA utilisés ou dont l’utilisation est prévue, puis les classer
selon les différentes catégories de risques de l’AI Act (interdits, haut
risque, usage général, etc).  
Ensuite, comme pour le RGPD, va également se poser la question de la
gouvernance et de la désignation des responsables de la conformité en matière
d’IA dans les entreprises, puisque l’on croise ici, plus que jamais, des
compétences juridiques et techniques. Toutes les entreprises concernées se
posent effectivement la question de savoir comment elles vont s’organiser en
interne pour mettre en place les bons process et être en conformité dans deux
ans.  
Enfin, on retrouve aussi des similitudes au niveau des sanctions… Une fois
entré en application, il y aura des sanctions plus importantes même que le
RGPD puisque les plus grosses sanctions pourront monter jusqu’à maximum 35
millions d’euros ou 7% du chiffre d’affaires mondial (pour les IA interdites).

— L’AI Act Game met l’utilisateur dans la peau d’une équipe de développement
logiciel qui doit développer un cas d’usage d’intelligence artificielle, et de
ce fait, doit choisir entre différents cas d’usage ?

C’est cela. Et en fonction du cas d’usage que le joueur choisit, il a accès
aux dispositions de l’AI Act qui l’encadre et il apprend quels sont les
critères pour rentrer dans telle ou telle catégorie. L’utilisateur va faire la
rencontre d’un petit robot qui va l’accompagner, qui va pouvoir également lui
donner quelques informations générales sur ce qu’est l’AI Act, comment le
texte est structuré, quelles sont les sanctions… Puis, l’utilisateur va avoir
le choix entre quatre mises en situation, cas d’usage, et il va pouvoir
naviguer pour découvrir les obligations qui s’appliqueraient à lui s’il était
en train vraiment de développer ce cas d’usage.

  * 

Le but du jeu est de parvenir à se mettre en conformité avec toutes les
obligations de l’AI Act, et pour l’utilisateur de jouer toutes les mises en
situation, couvrant la grande majorité des obligations du texte d’une façon
assez simple.

Cette connaissance globale du texte est nécessaire pour savoir si on est
concerné typiquement. L’impact du texte variera nécessairement en fonction du
secteur d’activité. Par exemple, si je suis une entreprise du secteur de
l’énergie, je vais être concerné par le texte parce que les systèmes utilisés
dans les infrastructures critiques figurent dans la liste des systèmes d’IA à
haut risque. En revanche, si je suis une entreprise dans le luxe, évidemment,
je ne vais pas être concernée au même titre, mais je pourrai l’être pour mes
applications RH, par exemple, parce qu’on les retrouve dans toutes les
entreprises. Si j’avais des projets d’IA pour évaluer mes candidats de façon
automatique, il va falloir que je mette en place tous ces processus pour me
mettre en conformité, peu importe mon secteur d’activité !

## AI Act : une nouvelle réglementation

— Pour parler un peu du texte lui-même, pouvez-vous nous résumer quelles sont
ces quatre catégories d’IA réglementées par l’AI Act ?

La première catégorie est celle des IA posant un risque inacceptable : ces IA-
là seront tout bonnement interdites. Parmi ces cas d’usage-là, on trouve
notamment les systèmes de reconnaissance faciale à distance et en temps réel
pour des finalités répressives. Si on ne remplit pas ces quatre critères-là
(reconnaissance faciale + à distance + en temps réel + à finalité répressive),
notre cas d’usage ne sera pas interdit, mais il serait quand même soumis à une
obligation de notification auprès d’une autorité pour avoir une autorisation.

La deuxième catégorie regroupe les systèmes d’IA à haut risque qui, eux, ne
seront pas interdits, mais seront soumis à une multitude de critères de
conformité très exigeants, ex-ante, c’est-à-dire qu’il faudra se mettre en
conformité avec des exigences très précises avant de mettre sur le marché le
système d’IA. On a par exemple des exigences sur la qualité des données,
imposant au fournisseur de documenter le choix des données qui ont été
utilisées pour l’entraînement des algorithmes, comment je me suis assuré
qu’elles n’étaient pas biaisées, qu’il n’y avait pas d’erreurs, etc. Ou encore
des exigences de contrôle humain : les systèmes d’IA à haut risque devront
être accompagnés de mesures qui permettent d’avoir toujours un humain qui
supervise le système, en capacité de prendre la main si jamais il observe
qu’il y a une erreur, ou de mettre un terme à l’opération du système si jamais
il commence à dérailler. Tout cela doit être documenté et faire l’objet de
processus de contrôle interne. Il y a en tout 7 grandes obligations à
respecter : la mise en place d’un système d’évaluation des risques, la
gouvernance des données, le contrôle humain, la transparence, la conservation
de journaux pour assurer la traçabilité du système, la documentation technique
et la précision, robustesse et sécurité du système.

  * 

Les entreprises vont donc devoir s’organiser, se doter de politiques internes,
organiser une gouvernance, comme ce qui a été fait pour le RGPD en réalité,
pour s’assurer que ces obligations soient respectées.

Pour savoir si un système d’IA est considéré comme à haut risque, il y a deux
possibilités : d’une part, cela concerne les IA intégrées à un produit déjà
soumis à un processus de certification par un tiers, au titre d’une
réglementation dans l’UE, comme l’aviation civile, les jouets, les véhicules à
moteur… D’autre part, l’AI Act dresse également une liste dans comprenant huit
grandes catégories de systèmes d’IA considérées comme à haut risque, par
exemple, les systèmes d’IA utilisés comme composants de sécurité dans les
infrastructures critiques, comme la fourniture d’énergie, la fourniture de
gaz, la gestion du trafic routier, mais également l’IA utilisée dans les
fonctions RH, afin de faire de la sélection automatique de CV ou de
l’évaluation de candidats par exemple.

Outre les systèmes interdits et les systèmes à haut risque, la troisième
catégorie dans l’AI Act regroupe les systèmes d’IA « à usage général », qui
renvoient en fait à tous les systèmes qui sont suffisamment généraux pour être
appliqués à de nombreuses applications concrètes. Par exemple, Chat GPT est
une application fondée sur un modèle de langue qui peut être utilisée par une
entreprise, autant pour générer des supports de communication que des
recommandations d’aide à la décision. Ce sont donc des modèles qui n’ont pas
une finalité précise et qui sont soumis à des règles spécifiques.  
Le fournisseur d’un modèle général ne peut pas forcément avoir connaissance de
tous les risques, de toutes les applications potentielles. Mais il est
important qu’il ait mis en place une documentation expliquant comment il a
construit le modèle, qu’il ait étudié quelles sont ses limites et ses
capacités, et qu’il ait fourni ces informations aux personnes qui vont les
déployer par la suite, les « déployeurs » ou les utilisateurs, qui eux, vont
appliquer le système à une finalité précise. Enfin, le fournisseur de tels
modèles devra également garantir la traçabilité des bases de données qui ont
été utilisées pour l’entraînement.

La quatrième catégorie dans l’AI Act correspond aux systèmes d’IA initialement
qualifiés « à risque limité » et désormais nommés « certains systèmes d’IA »,
qui, si je résume, recouvrent ceux qui interagissent avec les personnes, donc
les chatbots, par exemple, et les systèmes d’IA qui servent à générer ou
modifier du contenu, comme ceux permettant de créer des « deepfakes ». Là, les
obligations ne sont pas les mêmes que celles des systèmes d’IA à haut risque,
puisque il n’est question que de transparence. Il faut par exemple que
l’utilisateur sache qu’il est en train d’interagir avec un chatbot ou, si on
fait de la génération de contenu, que le contenu soit systématiquement tagué,
comme étant été généré par intelligence artificielle.

Et pour terminer, si j’ai un système d’IA qui ne rentre dans aucune de ces
catégories, je n’ai pas d’obligation contraignante au titre du règlement IA.
Il y a une incitation à adhérer un code de conduite, mais c’est aujourd’hui
non contraignant.

— Et la cybersécurité ? Des protections spécifiques sont-elles à prévoir pour
les systèmes d’IA ?

Oui. C’est le cas pour les IA à haut risque, qui doivent présenter un niveau
élevé de cybersécurité. Pour cette obligation, des standards techniques vont
être publiés par la Commission ou par les instances de normalisation
européennes, notamment pour définir les bonnes pratiques et les métriques à
mesurer en termes de précision et de sécurité des modèles et des systèmes.  
Pour les IA à usage général, les plus gros modèles, ceux qui présentent ce
qu’on appelle un risque systémique, seront soumis à des obligations
supplémentaires parmi lesquelles on trouvera, justement, des obligations plus
importantes en termes de cybersécurité : les fournisseurs devront notamment
tester le système contre de potentielles attaques, documenter les résultats et
en informer les utilisateurs.

## Télécom Paris : une caution scientifique

— Pour en revenir au jeu, alors que beaucoup d’acteurs privés vont sûrement
diffuser des guides ou des formations, en quoi le positionnement spécifique de
Télécom Paris, la grande école d’ingénieurs du numérique, est-il un gage de
crédibilité ?

Il y a une caution scientifique effectivement puisque tout le contenu du jeu a
été produit par des enseignants-chercheurs. Il semble assez légitime qu’une
grande école d’ingénieurs comme Télécom Paris, spécialisée sur le numérique,
qui est dotée d’un département de Sciences économiques et sociales (SES) dont
[Winston Maxwell](https://www.telecom-paris.fr/accord-ia-act-winston-maxwell)
et moi-même faisons partie, se positionne sur un sujet si important.

  * 

Comme je l’ai dit, la formation à l’éthique de l’IA et à la future
règlementation est essentielle à la fois pour nos élèves ainsi que pour le
grand public, les entreprises et acteurs publics, afin d’assurer le
développement d’une IA « digne de confiance » et respectueuse des règles
juridiques en Europe.

Notre volonté est d’initier une dynamique d’intelligence collective, à
l’Institut Polytechnique de Paris et auprès de nos partenaires. Nous allons
commencer par diffuser le jeu auprès de notre réseau, et de personnes qui
enseignent l’intelligence artificielle, qui pourront aussi l’utiliser, nous
faire des retours, afin de l’améliorer ensemble et construire quelque chose de
plus grand par la suite. C’est un support qui va vivre, qui va s’enrichir, au
fur et à mesure des publications et de l’entrée en application du texte.

Nous envisageons par la suite de le faire grandir en un projet plus ambitieux,
par exemple une plate-forme de formation en ligne sur l’AI Act, une
certification ou un « passeport AI Act » avec des contenus pédagogiques,
ludiques et accessibles.

On est au tout début de l’histoire et nous invitons toute personne intéressée
à nous contacter pour contribuer à ce beau projet !

[](https://www.telecom-paris.fr/fr/ideas/sommaire)

