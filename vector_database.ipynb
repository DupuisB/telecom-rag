{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Building a Retrieval-Augmented Generation (RAG) Application with LangChain and MistralAI\n",
    "\n",
    "This notebook demonstrates how to build a RAG application using LangChain and MistralAI to query over a dataset of markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import the necessary libraries and load your API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Ensure the API key is loaded\n",
    "assert mistral_api_key is not None, \"Please set your MISTRAL_API_KEY in the .env file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-documents",
   "metadata": {},
   "source": [
    "## 2. Load Markdown Documents\n",
    "\n",
    "Load all markdown files from the `data/` directory, including all subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load-docs-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1156 documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_documents(root_dir):\n",
    "    documents = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".md\"):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                loader = UnstructuredMarkdownLoader(filepath)\n",
    "                documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Load documents from the data directory\n",
    "docs = load_documents(\"data\")\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-documents",
   "metadata": {},
   "source": [
    "## 3. Split Documents into Chunks\n",
    "\n",
    "Split the documents into smaller chunks to improve retrieval efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "split-docs-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 12733 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(split_docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-embeddings",
   "metadata": {},
   "source": [
    "## 4. Create Embeddings for the Text Chunks\n",
    "\n",
    "Convert the text chunks into vector embeddings using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create-embeddings-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258562/1469870572.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/home/benjamin/Documents/IA/telecom-rag/env/lib64/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-vector-store",
   "metadata": {},
   "source": [
    "## 5. Build a Vector Store with FAISS\n",
    "\n",
    "Store the embeddings in a vector store to enable efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "build-vector-store-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created.\n"
     ]
    }
   ],
   "source": [
    "# Build the FAISS vector store from the documents and embeddings\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding)\n",
    "\n",
    "print(\"Vector store created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-retriever",
   "metadata": {},
   "source": [
    "## 6. Set Up a Retriever\n",
    "\n",
    "Create a retriever that uses the vector store to find relevant documents based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "setup-retriever-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Optional: Customize retriever settings\n",
    "retriever.search_kwargs['k'] = 50  # Number of documents to retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialize-llm",
   "metadata": {},
   "source": [
    "## 7. Initialize the MistralAI Language Model\n",
    "\n",
    "Set up the MistralAI language model to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initialize-llm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MistralAI language model\n",
    "llm = ChatMistralAI(model=\"mistral-small\", api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-retrievalqa-chain",
   "metadata": {},
   "source": [
    "## 8. Create a RetrievalQA Chain\n",
    "\n",
    "Combine the retriever and the language model into a chain that can handle queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "create-retrievalqa-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalQA chain created.\n"
     ]
    }
   ],
   "source": [
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-rag-application",
   "metadata": {},
   "source": [
    "## 9. Test the RAG Application\n",
    "\n",
    "Run a test query to see the RAG application in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "test-rag-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : Donnes moi la liste des filières de 2ème année.\n",
      "\n",
      "Answer: The list of 2nd year specializations includes:\n",
      "\n",
      "1. Economics and Digital Innovation\n",
      "2. Markets, Organization, Data, Strategy\n",
      "3. Artificial Intelligence and Data Science\n",
      "4. Image\n",
      "5. Data Science\n",
      "6. Signal Processing for Artificial Intelligence\n",
      "7. Applied Algebra\n",
      "8. Mathematics, Theoretical Computer Science and Operational Research\n",
      "9. Random Modeling and Scientific Computing\n",
      "10. Networks, Communications and Cybersecurity\n",
      "11. Cybersecurity\n",
      "12. Large Digital Infrastructures\n",
      "13. Mobile Networks and Internet of Things\n",
      "\n",
      "These specializations are composed of units of teaching (UE) in 2nd year, which open on options of 3rd year. The courses are 24 hours or 48 hours each. Students must choose 2 specializations among 14.\n"
     ]
    }
   ],
   "source": [
    "# Define a test query\n",
    "query = \"Quelles sont les UEs de la filières IGR ? Qui est le responsable de la filière ?\"\n",
    "\n",
    "# Run the query through the RetrievalQA chain\n",
    "result = qa_chain.run(query)\n",
    "\n",
    "# Display the result\n",
    "print(\"Query :\", query)\n",
    "print(\"\")\n",
    "print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "You've successfully built a RAG application that can answer queries based on your markdown documents. You can now expand upon this foundation to handle more complex queries or integrate additional features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
